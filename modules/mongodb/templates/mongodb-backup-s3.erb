#!/usr/bin/env python
from datetime import datetime
import boto3
import json
import random
import os
import shutil
import subprocess
import sys
import time

############################################
# Variables
############################################
backup_dir = '<%= @dir %>'
date_format = '%Y-%m-%d:%H:%M:%S'
mongo_server = '<%= @server %>'
key_fingerprint = '<%= @private_gpg_key_fingerprint %>'
s3_bucket = '<%= @s3_bucket %>'
standalone = '<%= @standalone %>'
start = datetime.now().strftime(date_format)
start_time = time.time()
file = 'mongodump-<%= @fqdn %>-' + start + '.tar.gz.gpg'
############################################

def logger(str):
    print datetime.now().strftime(date_format) + ': ' + str

if not standalone:
  logger('Find Secondary Mongo members')
  try:
      mongo_nodes = subprocess.check_output(["mongo",  mongo_server, "--quiet", "--eval", "printjson(rs.status().members.map(function(m) { return {'name':m.name, 'stateStr':m.stateStr} }))"])
  except OSError:
      print 'Trouble querying mongo server'

  nodes = json.loads(mongo_nodes)
  secondary_nodes = { x["name"] for x in nodes if x["stateStr"] == "SECONDARY" }

  backup_node = random.sample(secondary_nodes,1)
else:
  backup_node = ['localhost']
print "Backing up", backup_node[0]

logger('Start mongo backup')
try:
    subprocess.call(['mongodump', '-h', backup_node[0], '-o', backup_dir])
except OSError:
    print 'There was a problem backing up the mongo database'

logger('End mongo backup')

logger('Start compress and encrypt backups')

tar = subprocess.Popen(['tar', '-cvz', backup_dir], stdout=subprocess.PIPE)
gpg = subprocess.Popen(['gpg', '-e', '-o', backup_dir + '/' + file, '-r', key_fingerprint, '--cipher-algo', 'AES256', '--force-mdc'], stdin=tar.stdout, stdout=subprocess.PIPE)
tar.stdout.close()
gpg.communicate()

logger('End compress and encrypt backups')

s3 = boto3.resource('s3')

logger('Start upload file to S3')

s3_file = backup_dir + '/' + file
s3.Object(s3_bucket, s3_file).put(Body=open(s3_file, 'rb'))

logger('End upload file to S3')

logger('Start cleanup filesystem')

delete_tree = backup_dir + '/*'
[os.remove(os.path.join(backup_dir,f)) for f in os.listdir(backup_dir) if f.endswith(".gpg")]
shutil.rmtree(delete_tree)

logger('End cleanup filesystem')

end_time = time.time()
delta = end_time - start_time

print 'Script completed in     : %.1f seconds' % delta
